{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "81e46919",
   "metadata": {},
   "source": [
    "# Bag of Words (BoW) Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b2f0dd4",
   "metadata": {},
   "source": [
    "### Definition\n",
    "\n",
    "The bag of words representation is the matrix representation of the frequency of words per document from actual raw text data. The values inside the cells in the matrix representation can be filled-in two ways:\n",
    "1. We can either fill the cell with the frequency of a word (values $\\geq$ 0) \n",
    "2. Or we can fill the cell with either `0`, in case the word is not present in the document or `1`, in case the word is present. This approach is also known as the binary bag of words model.\n",
    "\n",
    "The frequency approach is more commonly used in practice and the popular `NLTK` library in PyThon uses the word frequency approach instead of binary values.\n",
    "\n",
    "### Steps\n",
    "\n",
    "In this notebook we'll learn how to perform some pre-processing steps before building a bag of words model. \n",
    "1. We have to lowercase all the words to bring every word in the universal casing else it will take “Machine” and “machine” as two separate words. \n",
    "2. We need to remove all punctuation from the vocabulary.\n",
    "3. We need to tokenize each document.\n",
    "4. Finally, remove all stop words from the documents.\n",
    "\n",
    "### Duration\n",
    "\n",
    "$\\approx$ 30minutes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2f9609c",
   "metadata": {},
   "source": [
    "## 1. Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5d73050a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "20810854",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/hamzasellak/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords') # you'll need to execute this for the first time \n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18acfb9c",
   "metadata": {},
   "source": [
    "## 2. Load Data\n",
    "\n",
    "We're going to build a bag of words model based on 10 sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b16c75d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Deep learning models analyze past data to make accurate predictions.', 'Artificial intelligence encompasses various techniques, including machine learning.', 'Machine learning algorithms discover patterns and make informed decisions.', 'The power of machine learning lies in its ability to learn from experience.', 'Artificial intelligence systems leverage machine learning for autonomous task completion.', 'Machine learning enables computers to adapt and improve their performance over time.', 'Analyzing historical data is crucial for machine learning algorithms to make accurate forecasts.', 'Artificial intelligence integrates machine learning for intelligent decision-making processes.', 'Machine learning algorithms autonomously uncover insights and trends in datasets.', 'The predictive capabilities of machine learning come from analyzing past data patterns.']\n"
     ]
    }
   ],
   "source": [
    "# Open the file in read mode\n",
    "with open(\"./data/documents.txt\", \"r\") as file:\n",
    "    # Read the contents of the file\n",
    "    lines = file.readlines()\n",
    "\n",
    "# Remove leading/trailing whitespace and create a list\n",
    "documents = [line.strip() for line in lines]\n",
    "\n",
    "# Print the documents\n",
    "print(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2de31569",
   "metadata": {},
   "source": [
    "## 2. Pre-process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5d5d5358",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(document, punctuation, stop_words):\n",
    "    \"\"\"\n",
    "    Prepares a document to be converted into a specific word representation model.\n",
    "    \n",
    "    Args:\n",
    "        document (str): document to be preprocessed\n",
    "        punctuation (str): punctuation marks to be removed from the document\n",
    "        stop_words (list): list of words to be removed from the document \n",
    "        \n",
    "    Returns:\n",
    "        preprocessed_document (list): final document ready to be converted into a specific word representation model\n",
    "    \"\"\"\n",
    "    \n",
    "    # Convert to lowercase\n",
    "    document = document.lower()\n",
    "    \n",
    "    # Remove punctuation marks\n",
    "    document = document.translate(str.maketrans('', '', punctuation))\n",
    "    \n",
    "    # Convert document into tokens\n",
    "    document = document.split()\n",
    "    \n",
    "    # Remove stop words\n",
    "    document = [word for word in document if word not in stop_words]\n",
    "    \n",
    "    # Join back words to make a sentence\n",
    "    document = \" \".join(document)\n",
    "    \n",
    "    return document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c8a6ebb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(before)D1 = Deep learning models analyze past data to make accurate predictions.\n",
      "(after)D1 = deep learning models analyze past data make accurate predictions\n",
      "-----------\n",
      "(before)D2 = Artificial intelligence encompasses various techniques, including machine learning.\n",
      "(after)D2 = artificial intelligence encompasses various techniques including machine learning\n",
      "-----------\n",
      "(before)D3 = Machine learning algorithms discover patterns and make informed decisions.\n",
      "(after)D3 = machine learning algorithms discover patterns make informed decisions\n",
      "-----------\n",
      "(before)D4 = The power of machine learning lies in its ability to learn from experience.\n",
      "(after)D4 = power machine learning lies ability learn experience\n",
      "-----------\n",
      "(before)D5 = Artificial intelligence systems leverage machine learning for autonomous task completion.\n",
      "(after)D5 = artificial intelligence systems leverage machine learning autonomous task completion\n",
      "-----------\n",
      "(before)D6 = Machine learning enables computers to adapt and improve their performance over time.\n",
      "(after)D6 = machine learning enables computers adapt improve performance time\n",
      "-----------\n",
      "(before)D7 = Analyzing historical data is crucial for machine learning algorithms to make accurate forecasts.\n",
      "(after)D7 = analyzing historical data crucial machine learning algorithms make accurate forecasts\n",
      "-----------\n",
      "(before)D8 = Artificial intelligence integrates machine learning for intelligent decision-making processes.\n",
      "(after)D8 = artificial intelligence integrates machine learning intelligent decisionmaking processes\n",
      "-----------\n",
      "(before)D9 = Machine learning algorithms autonomously uncover insights and trends in datasets.\n",
      "(after)D9 = machine learning algorithms autonomously uncover insights trends datasets\n",
      "-----------\n",
      "(before)D10 = The predictive capabilities of machine learning come from analyzing past data patterns.\n",
      "(after)D10 = predictive capabilities machine learning come analyzing past data patterns\n",
      "-----------\n"
     ]
    }
   ],
   "source": [
    "# Initialise list of pre-processed documents\n",
    "preprocessed_documents = []\n",
    "\n",
    "# Punctuation marks\n",
    "punctuation = '!\"#$%&\\()*+,-./:;<=>?@[\\\\]^_`{|}~' # inspired from string.punctuation\n",
    "\n",
    "# Stop words\n",
    "stop_words = stopwords.words(\"English\") + [str('a')]\n",
    "\n",
    "# Preprocessing\n",
    "preprocessed_documents = [preprocess(d, punctuation, stop_words) for d in documents]\n",
    "\n",
    "# Print original and pre-processed documents\n",
    "for i in range(len(documents)):\n",
    "    print(f\"(before)D{i+1} = {documents[i]}\")\n",
    "    print(f\"(after)D{i+1} = {preprocessed_documents[i]}\\n-----------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddc84cdf",
   "metadata": {},
   "source": [
    "### 3. Bag of Words\n",
    "\n",
    "We'll use the `CountVectorizer` function from `sklearn` package to create our bag of words model. This function take all documents (sentences) as input and convert them into a matrix representation where each cell will be filled by the frequency of each word in our vocabulary in each document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "41cab3de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 18)\t1\n",
      "  (0, 33)\t1\n",
      "  (0, 38)\t1\n",
      "  (0, 4)\t1\n",
      "  (0, 39)\t1\n",
      "  (0, 14)\t1\n",
      "  (0, 37)\t1\n",
      "  (0, 1)\t1\n",
      "  (0, 43)\t1\n",
      "  (1, 33)\t1\n",
      "  (1, 6)\t1\n",
      "  (1, 30)\t1\n",
      "  (1, 21)\t1\n",
      "  (1, 52)\t1\n",
      "  (1, 48)\t1\n",
      "  (1, 26)\t1\n",
      "  (1, 36)\t1\n",
      "  (2, 33)\t1\n",
      "  (2, 37)\t1\n",
      "  (2, 36)\t1\n",
      "  (2, 3)\t1\n",
      "  (2, 19)\t1\n",
      "  (2, 40)\t1\n",
      "  (2, 27)\t1\n",
      "  (2, 17)\t1\n",
      "  :\t:\n",
      "  (7, 33)\t1\n",
      "  (7, 6)\t1\n",
      "  (7, 30)\t1\n",
      "  (7, 36)\t1\n",
      "  (7, 29)\t1\n",
      "  (7, 31)\t1\n",
      "  (7, 16)\t1\n",
      "  (7, 45)\t1\n",
      "  (8, 33)\t1\n",
      "  (8, 36)\t1\n",
      "  (8, 3)\t1\n",
      "  (8, 8)\t1\n",
      "  (8, 51)\t1\n",
      "  (8, 28)\t1\n",
      "  (8, 50)\t1\n",
      "  (8, 15)\t1\n",
      "  (9, 33)\t1\n",
      "  (9, 39)\t1\n",
      "  (9, 14)\t1\n",
      "  (9, 36)\t1\n",
      "  (9, 40)\t1\n",
      "  (9, 5)\t1\n",
      "  (9, 44)\t1\n",
      "  (9, 9)\t1\n",
      "  (9, 10)\t1\n"
     ]
    }
   ],
   "source": [
    "vectorizer = CountVectorizer()\n",
    "bow_model = vectorizer.fit_transform(preprocessed_documents)\n",
    "print(bow_model) # returns the rows and column number of cells which have 1 as value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efe753de",
   "metadata": {},
   "source": [
    "Next we need to convert the output into an array format to create a (sparse) matrix representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "06439d7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 1 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0\n",
      "  0 1 1 1 0 0 0 1 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 1 0 0 0 1 0 0 1 0 0\n",
      "  1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1]\n",
      " [0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0 0\n",
      "  1 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 1 0 1\n",
      "  1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 1 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 1 0\n",
      "  1 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0]\n",
      " [0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 0 0 0 1 0 0\n",
      "  1 0 0 0 0 1 0 0 0 0 0 0 0 1 0 0 0]\n",
      " [0 1 0 1 0 1 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 1 0 0\n",
      "  1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 0 1 0 0\n",
      "  1 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0]\n",
      " [0 0 0 1 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 1 0 0\n",
      "  1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0]\n",
      " [0 0 0 0 0 1 0 0 0 1 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0\n",
      "  1 0 0 1 1 0 0 0 1 0 0 0 0 0 0 0 0]]\n",
      "Number of rows (documents): 10\n",
      "Number of columns (unique words): 53\n"
     ]
    }
   ],
   "source": [
    "matrix_representation = bow_model.toarray()\n",
    "print(matrix_representation)\n",
    "print(f\"Number of rows (documents): {matrix_representation.shape[0]}\")\n",
    "print(f\"Number of columns (unique words): {matrix_representation.shape[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e7170c9",
   "metadata": {},
   "source": [
    "### 4. From a Matrix Representation To Dataframe (optional)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8774f2a",
   "metadata": {},
   "source": [
    "We can convert the bag of words (matrix) to a `pandas.DataFrame` by assigning our unique words as columns and our documents as rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b84ddffa",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ability</th>\n",
       "      <th>accurate</th>\n",
       "      <th>adapt</th>\n",
       "      <th>algorithms</th>\n",
       "      <th>analyze</th>\n",
       "      <th>analyzing</th>\n",
       "      <th>artificial</th>\n",
       "      <th>autonomous</th>\n",
       "      <th>autonomously</th>\n",
       "      <th>capabilities</th>\n",
       "      <th>...</th>\n",
       "      <th>predictions</th>\n",
       "      <th>predictive</th>\n",
       "      <th>processes</th>\n",
       "      <th>systems</th>\n",
       "      <th>task</th>\n",
       "      <th>techniques</th>\n",
       "      <th>time</th>\n",
       "      <th>trends</th>\n",
       "      <th>uncover</th>\n",
       "      <th>various</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 53 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   ability  accurate  adapt  algorithms  analyze  analyzing  artificial  \\\n",
       "0        0         1      0           0        1          0           0   \n",
       "1        0         0      0           0        0          0           1   \n",
       "2        0         0      0           1        0          0           0   \n",
       "3        1         0      0           0        0          0           0   \n",
       "4        0         0      0           0        0          0           1   \n",
       "5        0         0      1           0        0          0           0   \n",
       "6        0         1      0           1        0          1           0   \n",
       "7        0         0      0           0        0          0           1   \n",
       "8        0         0      0           1        0          0           0   \n",
       "9        0         0      0           0        0          1           0   \n",
       "\n",
       "   autonomous  autonomously  capabilities  ...  predictions  predictive  \\\n",
       "0           0             0             0  ...            1           0   \n",
       "1           0             0             0  ...            0           0   \n",
       "2           0             0             0  ...            0           0   \n",
       "3           0             0             0  ...            0           0   \n",
       "4           1             0             0  ...            0           0   \n",
       "5           0             0             0  ...            0           0   \n",
       "6           0             0             0  ...            0           0   \n",
       "7           0             0             0  ...            0           0   \n",
       "8           0             1             0  ...            0           0   \n",
       "9           0             0             1  ...            0           1   \n",
       "\n",
       "   processes  systems  task  techniques  time  trends  uncover  various  \n",
       "0          0        0     0           0     0       0        0        0  \n",
       "1          0        0     0           1     0       0        0        1  \n",
       "2          0        0     0           0     0       0        0        0  \n",
       "3          0        0     0           0     0       0        0        0  \n",
       "4          0        1     1           0     0       0        0        0  \n",
       "5          0        0     0           0     1       0        0        0  \n",
       "6          0        0     0           0     0       0        0        0  \n",
       "7          1        0     0           0     0       0        0        0  \n",
       "8          0        0     0           0     0       1        1        0  \n",
       "9          0        0     0           0     0       0        0        0  \n",
       "\n",
       "[10 rows x 53 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# creating a dataframe from a bag of words' matrix representation\n",
    "bow_to_df = pd.DataFrame(bow_model.toarray(), columns = vectorizer.get_feature_names_out())\n",
    "bow_to_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc91fedb",
   "metadata": {},
   "source": [
    "## 5. Limitations\n",
    "\n",
    "### Notebook Limitations\n",
    "\n",
    "As we can observe from the `Dataframe`, we get a lot of redundant features (words) after building the bag of words model. For instance, features such as ‘analyze’ and ‘analyzing’, ‘autonomous’ and ‘autonomously’, ‘predictions’ and ‘predictive’ are just a few examples of many duplicate features. These features should be considered as redundant since they’re not giving us any extra information.\n",
    "\n",
    "Words such as ‘perfect’ and ‘perfection’ are considered equivalent for certain natural language processing tasks such as sentiment analysis: i.e., when our goal is to detect whether a document (sentence) reflects a positive or negative sentiment. Keeping separate but equivalent words may affect the performance of the machine learning algorithms. It will also increase the number of features.\n",
    "\n",
    "One way to solve this problem is by applying extra pre-processing steps such as stemming and lemmatization.\n",
    "\n",
    "### Model Limitations\n",
    "\n",
    "The bag of words model is a simple and popular approach used in natural language processing. However, it has several limitations including:\n",
    "\n",
    "- **Loss of word order:**** The bag of words model ignores the order and structure of words in a text. It treats each word as an independent entity, discarding the sequential and contextual information. As a result, it fails to capture the meaning and nuances conveyed by word order.\n",
    "\n",
    "- **Lack of semantic understanding:** The model treats words as isolated units without considering their semantic relationships. It fails to capture the meaning of phrases, idioms, or expressions that rely on the compositionality of words. Consequently, the model may struggle to differentiate between similar words with different meanings.\n",
    "\n",
    "- **Vocabulary size:** The bag of words model represents a text as a fixed-length vector, where each dimension corresponds to a unique word in the vocabulary. As a result, the vocabulary size can become large, leading to high-dimensional feature vectors. This can be computationally expensive and may require significant memory resources.\n",
    "\n",
    "- **Sparsity:** In large corpora or datasets, most words in the vocabulary may not appear frequently. This leads to sparse feature vectors with many zero entries, making it challenging to extract meaningful patterns and relationships between words.\n",
    "\n",
    "- **Out-of-vocabulary words:** Words that were not present in the training data or not part of the predefined vocabulary are usually ignored or handled separately. This can lead to a loss of information when encountering new or rare words that could be relevant in the text analysis.\n",
    "\n",
    "Despite these limitations, the bag of words model remains a useful baseline approach for many tasks. Researchers and practitioners have developed more advanced techniques, such as word embeddings and neural network-based models, to overcome some of these limitations and capture more nuanced information from text data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
